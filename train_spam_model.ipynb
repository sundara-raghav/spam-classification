{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa26a9f",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset\n",
    "\n",
    "Download the dataset from: https://www.kaggle.com/code/mfaisalqureshi/email-spam-detection-98-accuracy\n",
    "\n",
    "The dataset should be named `spam.csv` and placed in a `data/` folder.\n",
    "\n",
    "Alternative: You can use the dataset from Kaggle API:\n",
    "```bash\n",
    "kaggle datasets download -d balaka18/email-spam-classification-dataset-csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f25640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: If you don't have the file, download it from the Kaggle link mentioned above\n",
    "# The dataset typically has columns: 'v1' (label) and 'v2' (message)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Creating sample data for demonstration...\")\n",
    "    # Creating a minimal dataset for demonstration purposes\n",
    "    # In production, use the actual Kaggle dataset\n",
    "    sample_data = {\n",
    "        'v1': ['ham', 'spam', 'ham', 'spam', 'ham'] * 200,\n",
    "        'v2': [\n",
    "            'Hey, how are you doing today?',\n",
    "            'WINNER!! You have won $1000000! Click here now!!!',\n",
    "            'Let\\'s meet for coffee tomorrow',\n",
    "            'FREE FREE FREE! Buy now and get 90% discount!!!',\n",
    "            'Can you send me the report please?'\n",
    "        ] * 200\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\"Sample dataset created for demonstration\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b555af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant columns and rename them\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Drop any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Encode labels: spam=1, ham=0\n",
    "df['label_encoded'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "print(f\"\\nDataset after preprocessing:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb192ab",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing\n",
    "\n",
    "Clean and preprocess the email text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    - Convert to lowercase\n",
    "    - Remove special characters and digits\n",
    "    - Remove extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning complete!\")\n",
    "print(f\"\\nExample of cleaned text:\")\n",
    "print(f\"Original: {df['message'].iloc[0]}\")\n",
    "print(f\"Cleaned: {df['cleaned_message'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd12af7",
   "metadata": {},
   "source": [
    "## Step 3: Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = df['cleaned_message']\n",
    "y = df['label_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af60083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# TF-IDF converts text into numerical features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,  # Use top 3000 features\n",
    "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n",
    "    ngram_range=(1, 2)  # Use unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c1257",
   "metadata": {},
   "source": [
    "## Step 4: Train Multiple Models and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2586e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "best_model = results[best_model_name]['model']\n",
    "best_accuracy = results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b86c8",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from best model\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Calculate and display confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], \n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved as 'confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] * 100 for name in model_names]\n",
    "\n",
    "bars = plt.bar(model_names, accuracies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('Model Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.ylim([90, 100])\n",
    "\n",
    "# Add accuracy values on top of bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{acc:.2f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Model comparison saved as 'model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305732f",
   "metadata": {},
   "source": [
    "## Step 6: Save the Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and vectorizer\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': vectorizer,\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': best_accuracy\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open('spam_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"Model saved successfully as 'spam_model.pkl'\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8cfbf3",
   "metadata": {},
   "source": [
    "## Step 7: Test the Saved Model\n",
    "\n",
    "Load the model and test it with new email examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eda880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and use the model\n",
    "def load_spam_classifier():\n",
    "    \"\"\"\n",
    "    Load the saved spam classification model\n",
    "    Returns: model_data dictionary containing model and vectorizer\n",
    "    \"\"\"\n",
    "    with open('spam_model.pkl', 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    return model_data\n",
    "\n",
    "def classify_email(email_text, model_data):\n",
    "    \"\"\"\n",
    "    Classify a single email as spam or ham\n",
    "    \n",
    "    Args:\n",
    "        email_text: The email message to classify\n",
    "        model_data: Dictionary containing model and vectorizer\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prediction, probability)\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    cleaned = clean_text(email_text)\n",
    "    \n",
    "    # Vectorize\n",
    "    vectorized = model_data['vectorizer'].transform([cleaned])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model_data['model'].predict(vectorized)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    try:\n",
    "        probability = model_data['model'].predict_proba(vectorized)[0]\n",
    "        prob_spam = probability[1]\n",
    "    except:\n",
    "        prob_spam = None\n",
    "    \n",
    "    # Convert to label\n",
    "    label = 'spam' if prediction == 1 else 'inbox'\n",
    "    \n",
    "    return label, prob_spam\n",
    "\n",
    "# Load the model\n",
    "loaded_model_data = load_spam_classifier()\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model type: {loaded_model_data['model_name']}\")\n",
    "print(f\"Model accuracy: {loaded_model_data['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample emails\n",
    "test_emails = [\n",
    "    \"Hey! How are you? Want to grab lunch tomorrow?\",\n",
    "    \"CONGRATULATIONS!!! You've WON $1,000,000! Click here NOW to claim your prize!!!\",\n",
    "    \"Meeting scheduled for 3pm in conference room B\",\n",
    "    \"FREE VIAGRA!!! Buy now and get 90% discount! Limited time offer!!!\",\n",
    "    \"Can you please send me the quarterly report by end of day?\",\n",
    "    \"URGENT: Your account will be closed. Click here to verify your information NOW!!!\"\n",
    "]\n",
    "\n",
    "print(\"Testing with sample emails:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    label, prob = classify_email(email, loaded_model_data)\n",
    "    \n",
    "    print(f\"\\nEmail {i}:\")\n",
    "    print(f\"Text: {email[:60]}{'...' if len(email) > 60 else ''}\")\n",
    "    print(f\"Classification: {label.upper()}\")\n",
    "    if prob is not None:\n",
    "        print(f\"Spam Probability: {prob*100:.2f}%\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1b02b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Training Complete!\n",
    "\n",
    "**Outputs:**\n",
    "1. `spam_model.pkl` - Trained model with vectorizer\n",
    "2. `confusion_matrix.png` - Confusion matrix visualization\n",
    "3. `model_comparison.png` - Model accuracy comparison\n",
    "\n",
    "**Key Functions:**\n",
    "- `load_spam_classifier()` - Load the saved model\n",
    "- `classify_email(text, model_data)` - Classify new emails\n",
    "\n",
    "**Usage Example:**\n",
    "```python\n",
    "model_data = load_spam_classifier()\n",
    "label, probability = classify_email(\"Your email text here\", model_data)\n",
    "print(f\"Classification: {label}\")\n",
    "```\n",
    "\n",
    "The model is ready to be integrated into the backend API!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
